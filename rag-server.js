import "dotenv/config";
import express from "express";
import cors from "cors";

import multer from "multer";
import path from "path";
import { ChatPromptTemplate } from "@langchain/core/prompts";

import { GoogleGenAI } from "@google/genai";
import {
  ChatGoogleGenerativeAI,
  GoogleGenerativeAIEmbeddings,
} from "@langchain/google-genai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { Document } from "@langchain/core/documents";
import { RetrievalQAChain } from "langchain/chains";
import { execFile } from "child_process";
import fs from "fs";
import XLSX from "xlsx";

import {
  interpretForecast,
  interpretForecastGroq,
  scriptifyForPodcast,
} from "./services/js/interpretData.js";
import { generatePodcast } from "./services/js/podCastGenerator.js";

// ---------- Multer setup ----------
//
// SET STORAGE

var STORAGE = multer.diskStorage({
  destination: function (req, file, cb) {
    cb(null, "uploads");
  },

  filename: function (req, file, cb) {
    cb(null, file.fieldname + "-" + Date.now());
  },
});
const upload = multer({ storage: STORAGE });

// ---------- LLM Factory ----------
class LLMFactory {
  static createGemini() {
    return new ChatGoogleGenerativeAI({
      model: "gemini-2.0-flash-lite",
      temperature: 0.3,
      apiKey: process.env.GOOGLE_API_KEY,
    });
  }
}

// ---------- Embeddings Factory ----------
class EmbeddingsFactory {
  static createGemini() {
    return new GoogleGenerativeAIEmbeddings({
      model: "gemini-embedding-001",
      apiKey: process.env.GOOGLE_API_KEY,
    });
  }
}

class VectorStoreFactory {
  static async create(filePath = "./data/template_loi_finance.xlsx") {
    const workbook = XLSX.readFile(filePath);
    const sheetName = workbook.SheetNames[0];
    const sheet = workbook.Sheets[sheetName];
    const jsonData = XLSX.utils.sheet_to_json(sheet);

    const docs = jsonData.map(
      (row) =>
        new Document({
          pageContent: `R√©gion: ${row.R√©gion}\nAnn√©e: ${row.Ann√©e}\nBudget Sant√©: ${row.Budget_Sant√©}\nPopulation: ${row.Population}\nCroissance: ${row.Croissance}\nD√©penses Sant√©: ${row.D√©penses_Sant√©}`,
          metadata: { src: "loi_finance" },
        })
    );

    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
    });
    const splitDocs = await splitter.splitDocuments(docs);

    const embeddings = EmbeddingsFactory.createGemini();
    const vectorStore = new MemoryVectorStore(embeddings);
    await vectorStore.addDocuments(splitDocs);

    return vectorStore;
  }
}

// ---------- Initialise RAG ----------
let ragChain;
async function initRAG() {
  const llm = LLMFactory.createGemini();
  const vectorStore = await VectorStoreFactory.create();
  const prompt = ChatPromptTemplate.fromTemplate(`
    Tu es un assistant intelligent. Utilise les informations pertinentes pour r√©pondre √† la question suivante.

    Contexte :
    {context}

    Question : {query}
    R√©ponse :
  `);
  ragChain = RetrievalQAChain.fromLLM(llm, vectorStore.asRetriever(), {
    prompt,
  });
}

// ---------- Express server ----------
async function main() {
  await initRAG();
  const app = express();
  app.use(cors());
  //app.use(bodyParser.json());
  app.use(express.json());

  app.use("/podcast", express.static(path.join(process.cwd(), "podcast")));
  // app.use(bodyParser.json());
  app.use(express.json());

  // RAG endpoint
  /**
   * Endpoint POST /ask
   * Attend { question } dans le body.
   * Retourne { response: ... }
   */
  app.post("/ask", async (req, res) => {
    try {
      const { question } = req.body;
      console.log("body", req);
      if (!question) {
        return res.status(400).json({ error: "question manquante" });
      }

      // Appel IA
      const ai = new GoogleGenAI({
        apiKey: process.env.GEMINI_API_KEY,
      });

      const model = "gemma-3-27b-it";

      const prompt = `Tu es un expert en finances publiques. R√©ponds de fa√ßon br√®ve et claire √† la question suivante concernant des donn√©es budg√©taires (lois de finances) :\n\nQuestion : ${question}\nR√©ponse :`;

      const contents = [
        {
          role: "user",
          parts: [{ text: prompt }],
        },
      ];

      const responseStream = await ai.models.generateContentStream({
        model,
        config: {},
        contents,
      });

      let responseText = "";
      for await (const chunk of responseStream) {
        if (chunk.text) responseText += chunk.text;
      }

      res.json({ response: responseText.trim() });
    } catch (err) {
      console.error(err);
      res.status(500).json({ error: "Erreur serveur ou IA" });
    }
  });

  // Pr√©vision / Python endpoint
  app.post("/predict", upload.single("file"), async (req, res) => {
    if (!req.file) return res.status(400).json({ error: "Fichier manquant" });

    const filePath = path.resolve(req.file.path);

    execFile(
      "python3",
      ["services/python/merge_forecast.py", filePath],
      { maxBuffer: 1024 * 1024 * 10 }, // 10MB buffer
      async (err, stdout, stderr) => {
        // Supprimer le fichier temporaire
        fs.unlinkSync(filePath);

        if (err) {
          console.error(err, stderr);
          return res.status(500).json({ error: "Erreur pr√©vision Python" });
        }
        try {
          const data = JSON.parse(stdout);
          // üîπ Appel LLM pour interpr√©tation
          // const interpretation = await interpretForecast(data);

          const interpretation = await interpretForecastGroq(data);

          // üîπ Retourne les pr√©visions + l‚Äôinterpr√©tation
          res.json({
            forecast: data,
            interpretation,
          });
          // res.json(data);
        } catch (e) {
          console.error(e);
          res.status(500).json({ error: "Erreur parsing JSON Python" });
        }
      }
    );
  });

  // --- Endpoint Modifi√© ---
  app.post("/generatePodcast", express.json(), async (req, res) => {
    try {
      const { texte } = req.body;
      if (!texte) {
        return res.status(400).json({ error: "Champ 'texte' manquant" });
      }

      // 1. √âTAPE LLM : Transformer l'analyse brute en script de dialogue balis√©
      console.log(
        "√âtape 1: Transformation de l'analyse en script de dialogue..."
      );
      const dialogueScript = await scriptifyForPodcast(texte);

      if (!dialogueScript) {
        return res
          .status(500)
          .json({ error: "Le LLM n'a pas pu g√©n√©rer le script de dialogue." });
      }

      // 2. √âTAPE TTS : G√©n√©rer l'audio √† partir du script balis√©
      console.log("√âtape 2: G√©n√©ration du podcast audio √† partir du script...");

      // Ici, on appelle votre fonction generatePodcast qui est maintenant √† jour
      // (En supposant que vous ayez mis √† jour generatePodcast pour retourner UN seul fichier)
      const audioPath = await generatePodcast(dialogueScript, "podcast");

      res.json({
        success: true,
        message: "Podcast g√©n√©r√© avec succ√®s üéß",
        file: path.basename(audioPath), // Retourne uniquement le nom du fichier pour l'URL statique
      });
    } catch (err) {
      console.error("Erreur compl√®te du podcast:", err.message);
      res.status(500).json({
        error:
          "Erreur serveur lors de la g√©n√©ration du podcast: " + err.message,
      });
    }
  });

  app.listen(3000, () =>
    console.log("üöÄ Serveur RAG + pr√©vision sur http://localhost:3000")
  );
}
// async function interpretForecast(forecastData) {
//   if (!forecastData) return "Aucune donn√©e fournie.";

//   const ai = new GoogleGenAI({
//     apiKey: process.env.GEMINI_API_KEY, // Assure-toi que cette variable est bien d√©finie
//   });

//   const model = "gemma-3-27b-it";

//   // Pr√©parer le contexte textuel
//   const contextText = `
// Pr√©visions r√©gionales :
// ${forecastData.forecast_regional
//   .map(
//     (r) =>
//       `R√©gion: ${r.R√©gion}, Ann√©e: ${r.Ann√©e}, Budget Sant√©: ${r.Budget_Sant√©}, Population: ${r.Population}, Croissance: ${r.Croissance}, D√©penses pr√©vues: ${r.D√©penses_Pr√©dites}`
//   )
//   .join("\n")}

// Pr√©visions nationales :
// ${forecastData.forecast_national
//   .map((n) => `Ann√©e: ${n.Ann√©e}, D√©penses pr√©vues: ${n.D√©penses_Pr√©dites}`)
//   .join("\n")}
// `;

//   const contents = [
//     {
//       role: "user",
//       parts: [
//         {
//           text: `Tu es un expert en finances publiques. Analyse les pr√©visions ci-dessus et r√©dige une interpr√©tation claire pour un responsable de budget.

// ${contextText}`,
//         },
//       ],
//     },
//   ];

//   // Appel au mod√®le
//   const responseStream = await ai.models.generateContentStream({
//     model,
//     config: {},
//     contents,
//   });

//   // Lire le flux et concat√©ner le texte
//   let interpretation = "";
//   for await (const chunk of responseStream) {
//     if (chunk.text) interpretation += chunk.text;
//   }

//   return interpretation;
// }

main();
